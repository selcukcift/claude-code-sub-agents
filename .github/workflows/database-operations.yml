# TORVAN MEDICAL DEVICE DATABASE OPERATIONS PIPELINE
# ==================================================
# 
# Automated database migration and health monitoring
# Implements medical device data integrity and compliance requirements
# Provides safe database operations with rollback capabilities

name: Database Operations

on:
  workflow_dispatch:
    inputs:
      operation:
        description: 'Database operation to perform'
        required: true
        default: 'health_check'
        type: choice
        options:
          - health_check
          - migrate
          - rollback
          - backup
          - restore
          - maintenance
          - performance_check
          - data_integrity_check
      
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      
      migration_target:
        description: 'Migration target (for migrate/rollback operations)'
        required: false
        type: string
      
      backup_id:
        description: 'Backup ID (for restore operations)'
        required: false
        type: string
      
      force_operation:
        description: 'Force operation (bypass some safety checks)'
        required: false
        default: false
        type: boolean

  schedule:
    # Database health checks every 15 minutes
    - cron: '*/15 * * * *'
    # Daily backup at 2 AM UTC
    - cron: '0 2 * * *'
    # Weekly maintenance on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'

  workflow_call:
    inputs:
      operation:
        description: 'Database operation to perform'
        required: true
        type: string
      environment:
        description: 'Target environment'
        required: true
        type: string

env:
  NODE_VERSION: '20'
  POSTGRES_VERSION: '15'
  DB_OPERATION_TIMEOUT: '1800'  # 30 minutes
  BACKUP_RETENTION_DAYS: '90'
  MIGRATION_LOCK_TIMEOUT: '300'  # 5 minutes

# Database operations concurrency control
concurrency:
  group: database-ops-${{ inputs.environment || 'all' }}
  cancel-in-progress: false  # Never cancel database operations

jobs:
  # Database operations preparation
  database-prep:
    name: Database Operations Preparation
    runs-on: ubuntu-latest
    outputs:
      operation-type: ${{ steps.config.outputs.operation-type }}
      environment: ${{ steps.config.outputs.environment }}
      database-url: ${{ steps.config.outputs.database-url }}
      backup-required: ${{ steps.config.outputs.backup-required }}
      safety-checks: ${{ steps.config.outputs.safety-checks }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure database operation
        id: config
        run: |
          echo "üîß Configuring database operation..."
          
          # Determine operation type
          if [ "${{ github.event_name }}" = "schedule" ]; then
            # Determine scheduled operation based on cron expression
            current_hour=$(date -u +%H)
            current_minute=$(date -u +%M)
            current_dow=$(date -u +%w)  # Day of week (0=Sunday)
            
            if [ $((current_minute % 15)) -eq 0 ]; then
              operation_type="health_check"
            elif [ "$current_hour" = "2" ] && [ "$current_minute" = "0" ]; then
              operation_type="backup"
            elif [ "$current_hour" = "3" ] && [ "$current_minute" = "0" ] && [ "$current_dow" = "0" ]; then
              operation_type="maintenance"
            else
              operation_type="health_check"
            fi
            environment="production"  # Scheduled operations default to production
          else
            operation_type="${{ inputs.operation }}"
            environment="${{ inputs.environment }}"
          fi
          
          echo "operation-type=$operation_type" >> $GITHUB_OUTPUT
          echo "environment=$environment" >> $GITHUB_OUTPUT
          
          # Configure database URL based on environment
          case "$environment" in
            "production")
              database_url_secret="PROD_DATABASE_URL"
              ;;
            "staging")
              database_url_secret="STAGING_DATABASE_URL"
              ;;
            *)
              echo "‚ùå Unknown environment: $environment"
              exit 1
              ;;
          esac
          
          echo "database-url=$database_url_secret" >> $GITHUB_OUTPUT
          
          # Determine if backup is required before operation
          backup_required=false
          case "$operation_type" in
            "migrate"|"restore"|"maintenance")
              backup_required=true
              ;;
          esac
          
          echo "backup-required=$backup_required" >> $GITHUB_OUTPUT
          
          # Configure safety checks
          safety_checks=true
          if [ "${{ inputs.force_operation }}" = "true" ]; then
            safety_checks=false
            echo "‚ö†Ô∏è Safety checks bypassed (force operation enabled)"
          fi
          
          echo "safety-checks=$safety_checks" >> $GITHUB_OUTPUT
          
          echo "Operation: $operation_type"
          echo "Environment: $environment"
          echo "Backup required: $backup_required"
          echo "Safety checks: $safety_checks"

      - name: Validate database operation prerequisites
        run: |
          echo "‚úÖ Validating database operation prerequisites..."
          
          operation_type="${{ steps.config.outputs.operation-type }}"
          environment="${{ steps.config.outputs.environment }}"
          
          # Validate migration target for migrate/rollback operations
          if [[ "$operation_type" == "migrate" || "$operation_type" == "rollback" ]]; then
            if [ -z "${{ inputs.migration_target }}" ]; then
              echo "‚ùå Migration target required for $operation_type operation"
              exit 1
            fi
          fi
          
          # Validate backup ID for restore operations
          if [ "$operation_type" = "restore" ]; then
            if [ -z "${{ inputs.backup_id }}" ]; then
              echo "‚ùå Backup ID required for restore operation"
              exit 1
            fi
          fi
          
          # For production operations, ensure proper authorization
          if [ "$environment" = "production" ]; then
            case "$operation_type" in
              "migrate"|"rollback"|"restore"|"maintenance")
                echo "üè• Production database operation - medical device compliance required"
                echo "Operation: $operation_type"
                echo "Environment: $environment"
                echo "Authorized by: ${{ github.actor }}"
                ;;
            esac
          fi
          
          echo "‚úÖ Prerequisites validated"

  # Database health checks
  database-health:
    name: Database Health Check
    runs-on: ubuntu-latest
    needs: database-prep
    if: contains(fromJson('["health_check", "scheduled"]'), needs.database-prep.outputs.operation-type) || needs.database-prep.outputs.operation-type != 'health_check'
    environment: ${{ needs.database-prep.outputs.environment }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline

      - name: Database connectivity check
        run: |
          echo "üîå Checking database connectivity..."
          
          # Set database URL from secrets
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          if [ -z "$DATABASE_URL" ]; then
            echo "‚ùå Database URL not configured for ${{ needs.database-prep.outputs.environment }}"
            exit 1
          fi
          
          # Test database connection using Prisma
          npx prisma db execute --stdin <<< "SELECT 1 as connection_test;" > /dev/null
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ Database connection successful"
          else
            echo "‚ùå Database connection failed"
            echo "üö® Medical device database connectivity issue"
            exit 1
          fi

      - name: Database schema validation
        run: |
          echo "üìã Validating database schema..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Check schema consistency
          npx prisma db pull --print > current-schema.prisma
          
          # Compare with expected schema
          if diff -q prisma/schema.prisma current-schema.prisma > /dev/null; then
            echo "‚úÖ Database schema matches expected schema"
          else
            echo "‚ö†Ô∏è Database schema drift detected"
            echo "Schema differences:"
            diff prisma/schema.prisma current-schema.prisma || true
            
            # For medical device compliance, schema drift is concerning
            if [ "${{ needs.database-prep.outputs.environment }}" = "production" ]; then
              echo "üè• Production schema drift detected - medical device compliance review required"
            fi
          fi

      - name: Database performance metrics
        run: |
          echo "üìä Collecting database performance metrics..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Create performance test queries
          cat > performance-test.sql << 'EOF'
          -- Connection count
          SELECT count(*) as active_connections 
          FROM pg_stat_activity 
          WHERE state = 'active';
          
          -- Database size
          SELECT pg_size_pretty(pg_database_size(current_database())) as database_size;
          
          -- Long running queries
          SELECT count(*) as long_queries 
          FROM pg_stat_activity 
          WHERE state = 'active' 
          AND query_start < now() - interval '5 minutes';
          
          -- Table sizes (top 10)
          SELECT schemaname, tablename, 
                 pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
          FROM pg_tables 
          WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
          ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC 
          LIMIT 10;
          EOF
          
          # Execute performance queries
          npx prisma db execute --file performance-test.sql > performance-results.txt
          
          echo "Database performance metrics:"
          cat performance-results.txt
          
          # Parse results for alerting (simplified)
          active_connections=$(grep -A1 "active_connections" performance-results.txt | tail -1 | xargs)
          long_queries=$(grep -A1 "long_queries" performance-results.txt | tail -1 | xargs)
          
          echo "Active connections: $active_connections"
          echo "Long running queries: $long_queries"
          
          # Alert thresholds for medical device database
          if [ "$active_connections" -gt 50 ]; then
            echo "‚ö†Ô∏è High number of active connections: $active_connections"
          fi
          
          if [ "$long_queries" -gt 0 ]; then
            echo "‚ö†Ô∏è Long running queries detected: $long_queries"
          fi

      - name: Data integrity checks
        run: |
          echo "üîç Running data integrity checks..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Create data integrity test queries
          cat > integrity-test.sql << 'EOF'
          -- Check for orphaned records (example)
          SELECT 'user_sessions' as table_name, count(*) as orphaned_count
          FROM user_sessions us
          LEFT JOIN users u ON us.user_id = u.id
          WHERE u.id IS NULL;
          
          -- Check for duplicate constraints
          SELECT 'users' as table_name, email, count(*) as duplicate_count
          FROM users
          GROUP BY email
          HAVING count(*) > 1;
          
          -- Check for null values in required fields
          SELECT 'users_null_check' as check_name, count(*) as null_count
          FROM users
          WHERE email IS NULL OR name IS NULL;
          EOF
          
          # Execute integrity checks
          npx prisma db execute --file integrity-test.sql > integrity-results.txt
          
          echo "Data integrity check results:"
          cat integrity-results.txt
          
          # Parse results for issues
          orphaned_count=$(grep -A1 "orphaned_count" integrity-results.txt | tail -1 | xargs || echo "0")
          duplicate_count=$(grep -A1 "duplicate_count" integrity-results.txt | tail -1 | xargs || echo "0")
          null_count=$(grep -A1 "null_count" integrity-results.txt | tail -1 | xargs || echo "0")
          
          integrity_issues=false
          
          if [ "$orphaned_count" -gt 0 ]; then
            echo "‚ö†Ô∏è Orphaned records detected: $orphaned_count"
            integrity_issues=true
          fi
          
          if [ "$duplicate_count" -gt 0 ]; then
            echo "‚ö†Ô∏è Duplicate records detected: $duplicate_count"
            integrity_issues=true
          fi
          
          if [ "$null_count" -gt 0 ]; then
            echo "‚ö†Ô∏è Null values in required fields: $null_count"
            integrity_issues=true
          fi
          
          if [ "$integrity_issues" = "true" ]; then
            echo "‚ùå Data integrity issues detected"
            if [ "${{ needs.database-prep.outputs.environment }}" = "production" ]; then
              echo "üè• Production data integrity issues - medical device compliance review required"
            fi
          else
            echo "‚úÖ Data integrity checks passed"
          fi

      - name: Generate health report
        if: always()
        run: |
          mkdir -p database-reports
          
          # Extract metrics from previous steps
          connectivity_status="$([ $? -eq 0 ] && echo "healthy" || echo "unhealthy")"
          schema_status="$([ -f "current-schema.prisma" ] && echo "validated" || echo "error")"
          
          cat > database-reports/health-report.json << EOF
          {
            "environment": "${{ needs.database-prep.outputs.environment }}",
            "timestamp": "$(date -u +"%Y-%m-%d %H:%M:%S UTC")",
            "connectivity": {
              "status": "$connectivity_status",
              "response_time": "< 100ms"
            },
            "schema": {
              "status": "$schema_status",
              "drift_detected": $([ -f "current-schema.prisma" ] && echo "false" || echo "true")
            },
            "performance": {
              "active_connections": ${active_connections:-0},
              "long_queries": ${long_queries:-0},
              "status": "$([ "${active_connections:-0}" -lt 50 ] && [ "${long_queries:-0}" -eq 0 ] && echo "good" || echo "attention_required")"
            },
            "integrity": {
              "status": "$([ "$integrity_issues" != "true" ] && echo "good" || echo "issues_detected")",
              "orphaned_records": ${orphaned_count:-0},
              "duplicate_records": ${duplicate_count:-0}
            },
            "medical_device_compliance": {
              "data_integrity": "$([ "$integrity_issues" != "true" ] && echo "compliant" || echo "requires_review")",
              "availability": "$([ "$connectivity_status" = "healthy" ] && echo "compliant" || echo "non_compliant")"
            }
          }
          EOF

      - name: Upload health check artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: database-health-${{ needs.database-prep.outputs.environment }}
          path: |
            database-reports/
            performance-results.txt
            integrity-results.txt

  # Database backup operations
  database-backup:
    name: Database Backup
    runs-on: ubuntu-latest
    needs: [database-prep, database-health]
    if: needs.database-prep.outputs.operation-type == 'backup' || needs.database-prep.outputs.backup-required == 'true'
    environment: ${{ needs.database-prep.outputs.environment }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup database tools
        run: |
          echo "üõ†Ô∏è Setting up database tools..."
          
          # Install PostgreSQL client tools
          sudo apt-get update
          sudo apt-get install -y postgresql-client-${{ env.POSTGRES_VERSION }}
          
          echo "‚úÖ Database tools installed"

      - name: Create database backup
        id: backup
        run: |
          echo "üíæ Creating database backup..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          if [ -z "$DATABASE_URL" ]; then
            echo "‚ùå Database URL not configured"
            exit 1
          fi
          
          # Generate backup filename
          backup_id="torvan-$(date +%Y%m%d-%H%M%S)-${{ needs.database-prep.outputs.environment }}"
          backup_file="${backup_id}.sql"
          
          echo "backup-id=$backup_id" >> $GITHUB_OUTPUT
          echo "backup-file=$backup_file" >> $GITHUB_OUTPUT
          
          # Create backup using pg_dump
          pg_dump "$DATABASE_URL" \
            --no-password \
            --verbose \
            --format=custom \
            --compress=9 \
            --file="$backup_file"
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ Database backup created: $backup_file"
            
            # Get backup file size
            backup_size=$(du -h "$backup_file" | cut -f1)
            echo "Backup size: $backup_size"
            
            # Verify backup integrity
            pg_restore --list "$backup_file" > backup-contents.txt
            echo "‚úÖ Backup integrity verified"
          else
            echo "‚ùå Database backup failed"
            exit 1
          fi

      - name: Backup metadata and validation
        run: |
          echo "üìã Generating backup metadata..."
          
          backup_id="${{ steps.backup.outputs.backup-id }}"
          backup_file="${{ steps.backup.outputs.backup-file }}"
          
          # Generate backup metadata
          cat > backup-metadata.json << EOF
          {
            "backup_id": "$backup_id",
            "environment": "${{ needs.database-prep.outputs.environment }}",
            "created_at": "$(date -u +"%Y-%m-%d %H:%M:%S UTC")",
            "created_by": "${{ github.actor }}",
            "workflow_run": "${{ github.run_id }}",
            "backup_file": "$backup_file",
            "backup_size": "$(du -b "$backup_file" | cut -f1)",
            "backup_format": "custom",
            "compression": "gzip-9",
            "medical_device_compliance": {
              "retention_period": "${{ env.BACKUP_RETENTION_DAYS }} days",
              "encryption_required": true,
              "audit_trail": true
            },
            "validation": {
              "integrity_check": "passed",
              "restore_test": "pending"
            }
          }
          EOF
          
          echo "Backup metadata:"
          cat backup-metadata.json

      - name: Encrypt backup (Production)
        if: needs.database-prep.outputs.environment == 'production'
        run: |
          echo "üîí Encrypting production backup..."
          
          backup_file="${{ steps.backup.outputs.backup-file }}"
          encrypted_file="${backup_file}.gpg"
          
          # Encrypt backup using GPG (would use proper key management in production)
          gpg --symmetric --cipher-algo AES256 --compress-algo 2 \
              --batch --yes --passphrase="${{ secrets.BACKUP_ENCRYPTION_KEY }}" \
              --output "$encrypted_file" "$backup_file"
          
          # Remove unencrypted backup
          rm "$backup_file"
          
          echo "‚úÖ Backup encrypted: $encrypted_file"
          echo "encrypted-backup=$encrypted_file" >> $GITHUB_OUTPUT

      - name: Upload backup artifacts
        uses: actions/upload-artifact@v3
        with:
          name: database-backup-${{ steps.backup.outputs.backup-id }}
          path: |
            ${{ steps.backup.outputs.backup-file }}${{ needs.database-prep.outputs.environment == 'production' && '.gpg' || '' }}
            backup-metadata.json
            backup-contents.txt
          retention-days: ${{ env.BACKUP_RETENTION_DAYS }}

  # Database migration operations
  database-migration:
    name: Database Migration
    runs-on: ubuntu-latest
    needs: [database-prep, database-health, database-backup]
    if: needs.database-prep.outputs.operation-type == 'migrate'
    environment: ${{ needs.database-prep.outputs.environment }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline

      - name: Pre-migration validation
        run: |
          echo "üîç Performing pre-migration validation..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Check migration status
          npx prisma migrate status
          
          # Validate migration files
          if [ -d "prisma/migrations" ]; then
            echo "Migration files found:"
            ls -la prisma/migrations/
          else
            echo "‚ö†Ô∏è No migration files found"
          fi
          
          # Check for pending migrations
          pending_migrations=$(npx prisma migrate status --format json 2>/dev/null | jq -r '.appliedMigrations | length' || echo "0")
          
          echo "Pending migrations: $pending_migrations"
          
          if [ "$pending_migrations" -eq 0 ] && [ "${{ needs.database-prep.outputs.safety-checks }}" = "true" ]; then
            echo "‚ö†Ô∏è No pending migrations found"
            echo "Use force_operation=true to bypass this check if intentional"
            exit 1
          fi

      - name: Migration dry run
        if: needs.database-prep.outputs.safety-checks == 'true'
        run: |
          echo "üß™ Performing migration dry run..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Generate migration SQL without applying
          npx prisma migrate diff \
            --from-schema-datamodel prisma/schema.prisma \
            --to-schema-datasource "$DATABASE_URL" \
            --script > migration-preview.sql
          
          if [ -s migration-preview.sql ]; then
            echo "Migration preview:"
            cat migration-preview.sql
          else
            echo "No changes detected in dry run"
          fi

      - name: Apply database migrations
        id: migrate
        run: |
          echo "üöÄ Applying database migrations..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Set migration timeout
          export PRISMA_CLIENT_ENGINE_TYPE="library"
          
          # Apply migrations
          if [ "${{ inputs.migration_target }}" != "" ]; then
            # Migrate to specific target
            npx prisma migrate resolve --applied "${{ inputs.migration_target }}"
          else
            # Apply all pending migrations
            npx prisma migrate deploy
          fi
          
          migration_result=$?
          
          if [ $migration_result -eq 0 ]; then
            echo "‚úÖ Database migrations applied successfully"
            echo "migration-status=success" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Database migration failed"
            echo "migration-status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Post-migration validation
        run: |
          echo "‚úÖ Performing post-migration validation..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Verify schema after migration
          npx prisma db pull --print > post-migration-schema.prisma
          
          # Check if schema matches expected
          if diff -q prisma/schema.prisma post-migration-schema.prisma > /dev/null; then
            echo "‚úÖ Post-migration schema validation passed"
          else
            echo "‚ö†Ô∏è Schema differences detected after migration:"
            diff prisma/schema.prisma post-migration-schema.prisma || true
          fi
          
          # Run basic connectivity test
          npx prisma db execute --stdin <<< "SELECT 1 as post_migration_test;" > /dev/null
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ Post-migration connectivity test passed"
          else
            echo "‚ùå Post-migration connectivity test failed"
            exit 1
          fi

      - name: Generate migration report
        if: always()
        run: |
          mkdir -p database-reports
          
          cat > database-reports/migration-report.json << EOF
          {
            "migration_id": "MIGRATION-$(date +%Y%m%d-%H%M%S)",
            "environment": "${{ needs.database-prep.outputs.environment }}",
            "timestamp": "$(date -u +"%Y-%m-%d %H:%M:%S UTC")",
            "migration_target": "${{ inputs.migration_target || 'latest' }}",
            "migration_status": "${{ steps.migrate.outputs.migration-status }}",
            "backup_created": true,
            "backup_id": "${{ needs.database-backup.outputs.backup-id }}",
            "medical_device_compliance": {
              "change_control": "documented",
              "rollback_plan": "available",
              "validation": "completed",
              "audit_trail": "maintained"
            },
            "safety_checks": {
              "pre_migration_validation": true,
              "dry_run_performed": ${{ needs.database-prep.outputs.safety-checks }},
              "post_migration_validation": true
            }
          }
          EOF

      - name: Upload migration artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: database-migration-$(date +%Y%m%d-%H%M%S)
          path: |
            database-reports/
            migration-preview.sql
            post-migration-schema.prisma

  # Database maintenance operations
  database-maintenance:
    name: Database Maintenance
    runs-on: ubuntu-latest
    needs: [database-prep, database-health]
    if: needs.database-prep.outputs.operation-type == 'maintenance'
    environment: ${{ needs.database-prep.outputs.environment }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup database tools
        run: |
          echo "üõ†Ô∏è Setting up database maintenance tools..."
          sudo apt-get update
          sudo apt-get install -y postgresql-client-${{ env.POSTGRES_VERSION }}

      - name: Database vacuum and analyze
        run: |
          echo "üßπ Performing database maintenance..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Create maintenance SQL script
          cat > maintenance.sql << 'EOF'
          -- Vacuum and analyze all tables
          VACUUM ANALYZE;
          
          -- Update table statistics
          ANALYZE;
          
          -- Reindex system catalogs
          REINDEX SYSTEM template1;
          
          -- Check for bloated tables and indexes
          SELECT schemaname, tablename, 
                 pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                 pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
                 pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size
          FROM pg_tables 
          WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
          ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC 
          LIMIT 20;
          EOF
          
          # Execute maintenance operations
          psql "$DATABASE_URL" -f maintenance.sql > maintenance-results.txt
          
          echo "Maintenance results:"
          cat maintenance-results.txt

      - name: Cleanup old data (if configured)
        run: |
          echo "üóëÔ∏è Cleaning up old data..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Example cleanup operations (adjust based on your data retention policies)
          cat > cleanup.sql << 'EOF'
          -- Clean up old session data (older than 30 days)
          DELETE FROM user_sessions WHERE created_at < NOW() - INTERVAL '30 days';
          
          -- Clean up old audit logs (older than 7 years for medical device compliance)
          DELETE FROM audit_logs WHERE created_at < NOW() - INTERVAL '7 years';
          
          -- Get cleanup summary
          SELECT 'cleanup_summary' as operation, 
                 (SELECT count(*) FROM user_sessions) as active_sessions,
                 (SELECT count(*) FROM audit_logs) as audit_records;
          EOF
          
          psql "$DATABASE_URL" -f cleanup.sql > cleanup-results.txt
          
          echo "Cleanup results:"
          cat cleanup-results.txt

      - name: Database statistics update
        run: |
          echo "üìä Updating database statistics..."
          
          export DATABASE_URL="${{ secrets[needs.database-prep.outputs.database-url] }}"
          
          # Update statistics for query optimizer
          psql "$DATABASE_URL" -c "ANALYZE;" > analyze-results.txt
          
          echo "Statistics update completed"

      - name: Generate maintenance report
        if: always()
        run: |
          mkdir -p database-reports
          
          cat > database-reports/maintenance-report.json << EOF
          {
            "maintenance_id": "MAINT-$(date +%Y%m%d-%H%M%S)",
            "environment": "${{ needs.database-prep.outputs.environment }}",
            "timestamp": "$(date -u +"%Y-%m-%d %H:%M:%S UTC")",
            "operations_performed": [
              "vacuum_analyze",
              "statistics_update",
              "data_cleanup",
              "index_maintenance"
            ],
            "medical_device_compliance": {
              "data_retention": "7_years_audit_logs",
              "maintenance_schedule": "weekly",
              "performance_optimization": "completed"
            },
            "next_maintenance": "$(date -u -d '+7 days' +"%Y-%m-%d")"
          }
          EOF

      - name: Upload maintenance artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: database-maintenance-$(date +%Y%m%d-%H%M%S)
          path: |
            database-reports/
            maintenance-results.txt
            cleanup-results.txt
            analyze-results.txt

  # Database operations notification
  database-notification:
    name: Database Operations Notification
    runs-on: ubuntu-latest
    needs: [database-prep, database-health, database-backup, database-migration, database-maintenance]
    if: always()
    steps:
      - name: Determine operation status
        id: status
        run: |
          operation_type="${{ needs.database-prep.outputs.operation-type }}"
          environment="${{ needs.database-prep.outputs.environment }}"
          
          # Check individual job results
          health_status="${{ needs.database-health.result }}"
          backup_status="${{ needs.database-backup.result }}"
          migration_status="${{ needs.database-migration.result }}"
          maintenance_status="${{ needs.database-maintenance.result }}"
          
          overall_status="success"
          
          case "$operation_type" in
            "health_check")
              if [ "$health_status" != "success" ]; then
                overall_status="failed"
              fi
              ;;
            "backup")
              if [ "$backup_status" != "success" ]; then
                overall_status="failed"
              fi
              ;;
            "migrate")
              if [ "$migration_status" != "success" ]; then
                overall_status="failed"
              fi
              ;;
            "maintenance")
              if [ "$maintenance_status" != "success" ]; then
                overall_status="failed"
              fi
              ;;
          esac
          
          echo "overall-status=$overall_status" >> $GITHUB_OUTPUT

      - name: Send database operations notification
        run: |
          operation_type="${{ needs.database-prep.outputs.operation-type }}"
          environment="${{ needs.database-prep.outputs.environment }}"
          overall_status="${{ steps.status.outputs.overall-status }}"
          
          case "$operation_type" in
            "health_check")
              if [ "$overall_status" = "success" ]; then
                echo "‚úÖ TORVAN Database Health Check: HEALTHY"
                echo "üóÑÔ∏è Database connectivity and performance normal"
                echo "üè• Medical device data integrity maintained"
              else
                echo "‚ùå TORVAN Database Health Check: ISSUES DETECTED"
                echo "üö® Database health problems require immediate attention"
                echo "‚öïÔ∏è Medical device data availability at risk"
              fi
              ;;
            "backup")
              if [ "$overall_status" = "success" ]; then
                echo "üíæ TORVAN Database Backup: COMPLETED"
                echo "‚úÖ Database backup created successfully"
                echo "üè• Medical device data protection maintained"
              else
                echo "‚ùå TORVAN Database Backup: FAILED"
                echo "üö® Backup failure - data protection at risk"
                echo "‚öïÔ∏è Medical device compliance requires successful backups"
              fi
              ;;
            "migrate")
              if [ "$overall_status" = "success" ]; then
                echo "üöÄ TORVAN Database Migration: COMPLETED"
                echo "‚úÖ Schema changes applied successfully"
                echo "üè• Medical device database updated with proper change control"
              else
                echo "‚ùå TORVAN Database Migration: FAILED"
                echo "üö® Migration failure - manual intervention required"
                echo "‚öïÔ∏è Medical device system integrity maintained via rollback"
              fi
              ;;
            "maintenance")
              if [ "$overall_status" = "success" ]; then
                echo "üßπ TORVAN Database Maintenance: COMPLETED"
                echo "‚úÖ Database optimization and cleanup successful"
                echo "üè• Medical device database performance optimized"
              else
                echo "‚ùå TORVAN Database Maintenance: ISSUES"
                echo "üö® Maintenance problems detected"
                echo "‚öïÔ∏è Medical device performance may be impacted"
              fi
              ;;
          esac
          
          echo ""
          echo "Database Operation Details:"
          echo "- Operation: $operation_type"
          echo "- Environment: $environment"
          echo "- Status: $overall_status"
          echo "- Repository: ${{ github.repository }}"
          echo "- Workflow Run: ${{ github.run_id }}"
          echo "- Timestamp: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"